# Como Usar o Coletor de Dados P√∫blicos

## üìã Problemas Corrigidos

1. ‚úÖ **Erro `bs4` n√£o encontrado**: BeautifulSoup agora √© importado opcionalmente
2. ‚úÖ **Coleta DOU melhorada**: Busca em m√∫ltiplas p√°ginas e termos
3. ‚úÖ **Scripts standalone**: Para executar localmente sem depender do endpoint

## üöÄ Op√ß√µes de Execu√ß√£o

### Op√ß√£o 1: Via Endpoint API (Render)

Ap√≥s o deploy, use o endpoint:

```bash
POST /api/coletar-dados-publicos
{
  "limite_por_fonte": 50000,
  "integrar_banco": true,
  "salvar_csv": true
}
```

### Op√ß√£o 2: Script Standalone (Recomendado)

Execute diretamente no terminal:

```bash
# Coletar todas as fontes (50k registros)
python coletar_dados_publicos_standalone.py

# Coletar apenas DOU
python coletar_dados_publicos_standalone.py --apenas-dou --limite 10000

# Coletar apenas BigQuery
python coletar_dados_publicos_standalone.py --apenas-bigquery --limite 50000

# Salvar em CSV
python coletar_dados_publicos_standalone.py --salvar-csv

# Salvar em JSON
python coletar_dados_publicos_standalone.py --salvar-json

# Todas as op√ß√µes
python coletar_dados_publicos_standalone.py --limite 50000 --salvar-csv --salvar-json --integrar-banco
```

### Op√ß√£o 3: Validar BigQuery Primeiro

Antes de coletar, valide a conex√£o:

```bash
python validar_bigquery.py
```

Isso vai:
- ‚úÖ Verificar conex√£o BigQuery
- ‚úÖ Listar todas as tabelas dispon√≠veis
- ‚úÖ Testar queries nas tabelas principais
- ‚úÖ Mostrar quantos registros existem

## üìä Tabelas BigQuery Esperadas

Baseado na sua imagem do Google Cloud, estas tabelas devem estar dispon√≠veis:

- `NCMExportacao`
- `EmpresasImEx`
- `EmpresasMes7_2025`
- `Estabelecimentoscnpj`
- `municipio_exportacao`
- `municipio_importacao`
- `NCMImportacao`

## üîß Requisitos

1. **Depend√™ncias instaladas**:
   ```bash
   pip install -r backend/requirements.txt
   ```

2. **Vari√°veis de ambiente** (para BigQuery):
   - `GOOGLE_APPLICATION_CREDENTIALS_JSON`: Credenciais do Google Cloud (JSON string)

3. **Banco de dados** (opcional, se `--integrar-banco`):
   - `DATABASE_URL`: URL de conex√£o PostgreSQL

## üìù Logs

Os scripts geram logs autom√°ticos:
- Console: Sa√≠da em tempo real
- Arquivo: `coleta_publica_YYYYMMDD_HHMMSS.log`

## ‚ö†Ô∏è Troubleshooting

### Erro: "No module named 'bs4'"
```bash
pip install beautifulsoup4
```

### Erro: "GOOGLE_APPLICATION_CREDENTIALS_JSON n√£o configurada"
Configure a vari√°vel de ambiente ou use apenas `--apenas-dou`

### Erro: "PublicCompanyCollector n√£o est√° dispon√≠vel"
Verifique os logs do servidor Render ou execute o script standalone localmente

## üîó Cruzamento NCM + UF (ap√≥s coleta)

Ap√≥s os dados estarem no banco, o sistema pode executar o **cruzamento** entre:
- Empresas **importadoras** e **exportadoras**
- Por **NCM** e **UF** (munic√≠pio/estado)
- Resultados salvos na tabela `empresas_recomendadas`

### Via API
- **POST /api/coletar-dados-publicos** com `"executar_cruzamento": true` (padr√£o) ‚Äî coleta e depois cruza.
- **POST /api/cruzamento-ncm-uf** ‚Äî executa apenas o cruzamento (dados j√° no banco).

### Via script
```bash
python coletar_dados_publicos_standalone.py --limite 5000 --integrar-banco --executar-cruzamento
```

## üéØ Pr√≥ximos Passos

1. **Validar BigQuery**: Execute `python validar_bigquery.py`
2. **Testar coleta local**: Execute `python coletar_dados_publicos_standalone.py --limite 1000 --salvar-csv`
3. **Aguardar deploy**: Ap√≥s deploy no Render, testar endpoint
4. **Coleta completa**: Executar com `--limite 50000` e `--executar-cruzamento` para coleta + cruzamento
