# Como Configurar BigQuery

## ‚ö†Ô∏è Erro Atual

Se voc√™ est√° vendo:
```
‚ùå GOOGLE_APPLICATION_CREDENTIALS_JSON n√£o configurada
```

Isso significa que a vari√°vel de ambiente n√£o est√° configurada localmente.

## üì¶ Instalar depend√™ncias (PowerShell)

**N√£o cole blocos de markdown (```) no terminal.** Use um comando por vez:

```powershell
cd C:\Users\User\Desktop\Cursor\Projetos\projeto_comex
python -m pip install python-dotenv google-cloud-bigquery google-auth loguru --quiet
python validar_bigquery.py
```

Se o script mostrar "OP√á√ïES DE SA√çDA", escolha uma das alternativas (configurar .env, usar --apenas-dou, etc.).

## üîß Solu√ß√£o: Configurar Vari√°vel de Ambiente

### Op√ß√£o 1: PowerShell (Tempor√°rio - apenas nesta sess√£o)

```powershell
# Substitua {SEU_JSON_AQUI} pelo conte√∫do do arquivo JSON de credenciais
$env:GOOGLE_APPLICATION_CREDENTIALS_JSON = '{"type":"service_account","project_id":"...","private_key":"..."}'

# Testar
python validar_bigquery.py
```

### Op√ß√£o 2: Arquivo .env (Recomendado)

1. Crie ou edite o arquivo `.env` **na pasta do projeto** ou **dentro de `backend/`** (o sistema procura nos dois lugares):
   - `projeto_comex/.env` ou
   - `projeto_comex/backend/.env`

2. O JSON pode estar em **uma linha** ou em **v√°rias linhas**; o script l√™ os dois formatos.

3. **Nunca fa√ßa commit do `.env` no GitHub** ‚Äî ele j√° est√° no `.gitignore`. As chaves do BigQuery n√£o devem subir para o reposit√≥rio.

### Op√ß√£o 3: Configurar no Render (Para produ√ß√£o)

No dashboard do Render:
1. V√° em **Environment** ‚Üí **Environment Variables**
2. Adicione: `GOOGLE_APPLICATION_CREDENTIALS_JSON`
3. Cole o JSON completo das credenciais

## üìã Como Obter as Credenciais

1. Acesse: https://console.cloud.google.com/
2. Selecione o projeto: `liquid-receiver-483923-n6`
3. V√° em **IAM & Admin** ‚Üí **Service Accounts**
4. Crie ou selecione uma service account
5. V√° em **Keys** ‚Üí **Add Key** ‚Üí **Create new key** ‚Üí **JSON**
6. Baixe o arquivo JSON
7. Use o conte√∫do completo do JSON como valor da vari√°vel

## ‚ùå Erro 403 - Permiss√£o (bigquery.jobs.create)

Se aparecer:

```
403 Access Denied: User does not have bigquery.jobs.create permission in project liquid-receiver-483923-n6
```

A **service account** est√° autenticada, mas **n√£o tem permiss√£o para executar consultas** no BigQuery. √â preciso conceder a role **BigQuery Job User** (ou **BigQuery User**):

1. Acesse: https://console.cloud.google.com/iam-admin/iam?project=liquid-receiver-483923-n6  
2. Na lista **Principais**, localize o e-mail da service account (ex: `comex-bigquery@liquid-receiver-483923-n6.iam.gserviceaccount.com`).  
3. Clique no **l√°pis (Editar)** ao lado dela.  
4. Clique em **+ ADICIONAR OUTRA FUN√á√ÉO**.  
5. Busque e selecione **BigQuery Job User** (ou **BigQuery User** para permiss√£o mais ampla).  
6. Clique em **Salvar**.  

A propaga√ß√£o pode levar 1‚Äì2 minutos. Depois, rode de novo o comando de coleta.

### 403 continua mesmo ap√≥s dar permiss√£o ‚Äì o que conferir

0. **Confirmar qual conta est√° em uso**  
   Na pasta do projeto, rode: `python verificar_conta_bigquery.py`. Anote o **client_email** e o **project_id**. No IAM voc√™ deve editar exatamente esse e-mail e nesse projeto.

1. **Projeto certo no Google Cloud**  
   No topo da p√°gina do Console, abra o seletor de projeto e confira se est√° em **liquid-receiver-483923-n6** (o ID do projeto, n√£o s√≥ o nome ‚ÄúMy First Project‚Äù). A role precisa estar nesse projeto.

2. **Mesma service account**  
   O erro 403 agora mostra o **e-mail exato** da conta em uso. Voc√™ tamb√©m pode rodar `python verificar_conta_bigquery.py` na pasta do projeto para ver **client_email** e **project_id**. No IAM, a role **BigQuery Job User** (ou **BigQuery User**) deve estar nesse **mesmo** e-mail (n√£o em outra service account).

3. **Usar a role ‚ÄúBigQuery User‚Äù**  
   Se j√° deu **BigQuery Job User** e ainda d√° 403, adicione tamb√©m (ou troque por) **BigQuery User** na mesma service account. **BigQuery User** inclui permiss√£o para criar jobs e acessar dados.

4. **Aguardar e testar de novo**  
   √Äs vezes o IAM demora 5‚Äì10 minutos. Espere um pouco e rode de novo:
   ```powershell
   cd C:\Users\User\Desktop\Cursor\Projetos\projeto_comex
   python coletar_dados_publicos_standalone.py --apenas-bigquery --limite 5000 --integrar-banco --executar-cruzamento
   ```

5. **Remover e recolocar a role**  
   No IAM, edite a service account, remova a fun√ß√£o **BigQuery Job User** (ou **BigQuery User**), salve, depois adicione de novo e salve. Isso pode for√ßar a atualiza√ß√£o das permiss√µes.

6. **403 mesmo com as roles corretas no IAM**  
   - **API do BigQuery:** Em **APIs e servi√ßos** ‚Üí **Biblioteca**, procure "BigQuery API" e confira se est√° **habilitada** para o projeto `liquid-receiver-483923-n6`.  
   - **Propaga√ß√£o:** Altera√ß√µes no IAM podem levar 5‚Äì10 minutos. Espere e rode o script de novo.  
   - O coletor j√° usa o **projeto** das credenciais explicitamente ao criar o cliente BigQuery.

### 403 "permission to query table" (ler dados da tabela/dataset)

Se o erro for **"User does not have permission to query table ... Projeto_Comex.EmpresasImEx"** (ou outra tabela), a conta pode **criar jobs** mas n√£o pode **ler os dados** do dataset. Conceda **BigQuery Data Viewer** no dataset:

1. No Console: **BigQuery** ‚Üí **Explorador** (painel esquerdo).  
2. Localize o dataset **Projeto_Comex** no projeto `liquid-receiver-483923-n6`.  
3. Clique nos **tr√™s pontinhos** ao lado de **Projeto_Comex** ‚Üí **Compartilhar** (ou **Gerenciar permiss√µes do dataset**).  
4. **Adicionar principal** ‚Üí cole o e-mail da service account (ex: `comex-bigquery@liquid-receiver-483923-n6.iam.gserviceaccount.com`).  
5. **Fun√ß√£o:** **BigQuery Data Viewer** (Visualizador de dados do BigQuery).  
6. Salvar.

## üìã Usar as CONSULTAS (n√£o as tabelas do dataset)

O **dataset Projeto_Comex n√£o cont√©m as tabelas** que o coletor precisa; os dados est√£o nas **consultas salvas** no BigQuery (EmpresasImEx, NCMExportacao, NCMImportacao, etc.). O script **s√≥ executa o SQL dessas consultas**; n√£o l√™ tabelas do dataset.

1. No BigQuery (Explorador), abra cada **consulta salva** (EmpresasImEx, NCMExportacao, NCMImportacao, ou a consulta que j√° une tudo).  
2. Copie o **SQL** da consulta.  
3. Edite `backend/data_collector/bigquery_queries.json` e cole cada SQL como item do array `"queries"` (ex: `"queries": [ "SELECT ...", "SELECT ..." ]`).  
   - Se voc√™ tiver **uma** consulta que j√° retorna empresa + NCM + valor, use s√≥ ela.  
   - Se tiver **v√°rias** consultas, cada uma deve retornar colunas compat√≠veis: `empresa_nome`, `cnpj`, `ncm`, `estado`, `municipio`, `tipo_operacao`, `data_operacao`, `valor_fob`, `quantidade`, `peso_kg`.  
4. **Ou** defina a vari√°vel de ambiente `BIGQUERY_QUERIES_JSON` com um array JSON de strings SQL.  
5. Se n√£o houver consultas configuradas, o script avisa e n√£o coleta nada (n√£o usa tabelas do dataset).  
6. Na SQL voc√™ pode usar `@limite`; o script passa o limite da coleta.

**Alternativa:** salve as consultas como **views** ou **tabelas** no dataset Projeto_Comex e avise para ajustarmos o script para referenci√°-las.

## ‚úÖ Testar Configura√ß√£o

Ap√≥s configurar, teste:

```bash
python validar_bigquery.py
```

Deve mostrar:
- ‚úÖ Conectado ao BigQuery
- ‚úÖ Lista de tabelas
- ‚úÖ Contagem de registros

## üöÄ Pr√≥ximos Passos

Ap√≥s validar BigQuery:
1. Execute: `python coletar_dados_publicos_standalone.py --apenas-bigquery --limite 1000`
2. Ou teste o endpoint no Render ap√≥s deploy
