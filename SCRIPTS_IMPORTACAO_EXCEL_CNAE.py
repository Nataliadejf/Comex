"""
Scripts de Importa√ß√£o de Arquivos Excel e CNAE
===============================================

Este arquivo cont√©m os endpoints completos para importa√ß√£o de:
1. Arquivos Excel com dados de importa√ß√£o/exporta√ß√£o (Comex)
2. Arquivos CNAE Excel

Endpoints:
- POST /upload-e-importar-excel
- POST /upload-e-importar-cnae

Caracter√≠sticas:
- Processamento em background para evitar timeout
- Bulk inserts otimizados
- Processamento em batches
- Logs detalhados de progresso
"""

import threading
import tempfile
import os
import re
from datetime import date
from pathlib import Path

import pandas as pd
from fastapi import FastAPI, Depends, HTTPException, UploadFile, File
from sqlalchemy.orm import Session

from database.models import OperacaoComex, TipoOperacao, CNAEHierarquia
from database.database import SessionLocal
from loguru import logger


# ============================================================================
# ENDPOINT 1: Upload e Importar Excel (Importa√ß√£o/Exporta√ß√£o)
# ============================================================================

@app.post("/upload-e-importar-excel", tags=["importacao"])
async def upload_e_importar_excel(
    arquivo: UploadFile = File(..., description="Arquivo Excel (.xlsx ou .xls) para importar"),
    db: Session = Depends(get_db)
):
    """
    Faz upload de um arquivo Excel e importa automaticamente para o banco de dados.
    
    Aceita arquivos .xlsx e .xls via upload direto.
    OTIMIZADO: Usa bulk inserts e processamento em background para evitar timeout.
    
    IMPORTANTE: Retorna resposta imediata. O processamento continua em background.
    Verifique os logs do Render para acompanhar o progresso.
    
    Colunas esperadas no Excel:
    - C√≥digo NCM
    - Descri√ß√£o NCM
    - UF do Produto
    - Pa√≠ses
    - M√™s
    - Exporta√ß√£o - 2025 - Valor US$ FOB (ou Exporta√ß√£o - Valor US$ FOB)
    - Exporta√ß√£o - 2025 - Quilograma L√≠quido (ou Exporta√ß√£o - Quilograma L√≠quido)
    - Importa√ß√£o - 2025 - Valor US$ FOB (ou Importa√ß√£o - Valor US$ FOB)
    - Importa√ß√£o - 2025 - Quilograma L√≠quido (ou Importa√ß√£o - Quilograma L√≠quido)
    """
    import threading
    
    try:
        # Validar extens√£o do arquivo
        nome_arquivo = arquivo.filename.lower()
        if not (nome_arquivo.endswith('.xlsx') or nome_arquivo.endswith('.xls')):
            raise HTTPException(
                status_code=400,
                detail="Arquivo deve ser Excel (.xlsx ou .xls)"
            )
        
        logger.info(f"üì§ Recebendo upload do arquivo: {arquivo.filename}")
        
        # Salvar arquivo temporariamente
        with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx' if nome_arquivo.endswith('.xlsx') else '.xls') as tmp_file:
            conteudo = await arquivo.read()
            tmp_file.write(conteudo)
            caminho_temp = tmp_file.name
        
        logger.info(f"‚úÖ Arquivo salvo temporariamente: {caminho_temp}")
        
        # Processar em background para evitar timeout do Render (30s)
        def processar_em_background():
            db_bg = SessionLocal()
            try:
                logger.info(f"üîÑ Processamento em background iniciado para: {arquivo.filename}")
                
                # Ler Excel
                logger.info("üìñ Lendo arquivo Excel...")
                df = pd.read_excel(caminho_temp)
                logger.info(f"‚úÖ Arquivo lido: {len(df)} linhas, {len(df.columns)} colunas")
                
                # Detectar ano do nome do arquivo
                ano = 2025  # Default
                ano_match = re.search(r'20\d{2}', arquivo.filename)
                if ano_match:
                    ano = int(ano_match.group())
                
                stats = {
                    "arquivo": arquivo.filename,
                    "total_registros": 0,
                    "importacoes": 0,
                    "exportacoes": 0,
                    "erros": []
                }
                
                # Preparar lista para bulk insert
                operacoes_para_inserir = []
                meses_map = {
                    'janeiro': 1, 'fevereiro': 2, 'mar√ßo': 3, 'marco': 3,
                    'abril': 4, 'maio': 5, 'junho': 6,
                    'julho': 7, 'agosto': 8, 'setembro': 9,
                    'outubro': 10, 'novembro': 11, 'dezembro': 12
                }
                
                # Processar em batches para melhor performance
                batch_size = 500
                total_rows = len(df)
                
                logger.info(f"üîÑ Processando {total_rows} linhas em batches de {batch_size}...")
                
                for batch_start in range(0, total_rows, batch_size):
                    batch_end = min(batch_start + batch_size, total_rows)
                    batch_df = df.iloc[batch_start:batch_end]
                    
                    for idx, row in batch_df.iterrows():
                        try:
                            # Extrair dados b√°sicos
                            ncm = str(row.get('C√≥digo NCM', '')).strip() if pd.notna(row.get('C√≥digo NCM')) else None
                            if not ncm or len(ncm) < 4:
                                continue
                            
                            ncm_normalizado = ncm[:8] if len(ncm) >= 8 else ncm.zfill(8)
                            descricao = str(row.get('Descri√ß√£o NCM', '')).strip()[:500] if pd.notna(row.get('Descri√ß√£o NCM')) else ''
                            uf = str(row.get('UF do Produto', '')).strip()[:2] if pd.notna(row.get('UF do Produto')) else None
                            pais = str(row.get('Pa√≠ses', '')).strip() if pd.notna(row.get('Pa√≠ses')) else None
                            
                            # Processar m√™s
                            mes_str = str(row.get('M√™s', '')).strip() if pd.notna(row.get('M√™s')) else ''
                            mes = None
                            if mes_str:
                                match = re.search(r'(\d{1,2})', mes_str)
                                if match:
                                    mes = int(match.group(1))
                                else:
                                    for nome, num in meses_map.items():
                                        if nome in mes_str.lower():
                                            mes = num
                                            break
                            
                            if not mes or mes < 1 or mes > 12:
                                mes = 1  # Default
                            
                            data_operacao = date(ano, mes, 1)
                            mes_referencia = f"{ano}-{mes:02d}"
                            
                            # Processar EXPORTA√á√ÉO
                            valor_exp = row.get('Exporta√ß√£o - 2025 - Valor US$ FOB', 0) or row.get('Exporta√ß√£o - Valor US$ FOB', 0) or row.get('Valor Exporta√ß√£o', 0)
                            peso_exp = row.get('Exporta√ß√£o - 2025 - Quilograma L√≠quido', 0) or row.get('Exporta√ß√£o - Quilograma L√≠quido', 0) or row.get('Peso Exporta√ß√£o', 0)
                            
                            if pd.notna(valor_exp) and float(valor_exp) > 0:
                                operacoes_para_inserir.append({
                                    'ncm': ncm_normalizado,
                                    'descricao_produto': descricao,
                                    'tipo_operacao': TipoOperacao.EXPORTACAO,
                                    'uf': uf,
                                    'pais_origem_destino': pais,
                                    'valor_fob': float(valor_exp),
                                    'peso_liquido_kg': float(peso_exp) if pd.notna(peso_exp) else 0,
                                    'data_operacao': data_operacao,
                                    'mes_referencia': mes_referencia,
                                    'arquivo_origem': arquivo.filename
                                })
                                stats["exportacoes"] += 1
                                stats["total_registros"] += 1
                            
                            # Processar IMPORTA√á√ÉO
                            valor_imp = row.get('Importa√ß√£o - 2025 - Valor US$ FOB', 0) or row.get('Importa√ß√£o - Valor US$ FOB', 0) or row.get('Valor Importa√ß√£o', 0)
                            peso_imp = row.get('Importa√ß√£o - 2025 - Quilograma L√≠quido', 0) or row.get('Importa√ß√£o - Quilograma L√≠quido', 0) or row.get('Peso Importa√ß√£o', 0)
                            
                            if pd.notna(valor_imp) and float(valor_imp) > 0:
                                operacoes_para_inserir.append({
                                    'ncm': ncm_normalizado,
                                    'descricao_produto': descricao,
                                    'tipo_operacao': TipoOperacao.IMPORTACAO,
                                    'uf': uf,
                                    'pais_origem_destino': pais,
                                    'valor_fob': float(valor_imp),
                                    'peso_liquido_kg': float(peso_imp) if pd.notna(peso_imp) else 0,
                                    'data_operacao': data_operacao,
                                    'mes_referencia': mes_referencia,
                                    'arquivo_origem': arquivo.filename
                                })
                                stats["importacoes"] += 1
                                stats["total_registros"] += 1
                        
                        except Exception as e:
                            logger.error(f"Erro ao processar linha {idx}: {e}")
                            stats["erros"].append(f"Linha {idx}: {str(e)}")
                            continue
                    
                    # Log de progresso
                    if batch_end % 1000 == 0 or batch_end == total_rows:
                        logger.info(f"  üìä Processadas {batch_end}/{total_rows} linhas... ({len(operacoes_para_inserir)} opera√ß√µes preparadas)")
                
                # Bulk insert otimizado - inserir em chunks
                logger.info(f"üíæ Inserindo {len(operacoes_para_inserir)} opera√ß√µes no banco...")
                insert_chunk_size = 1000
                
                for i in range(0, len(operacoes_para_inserir), insert_chunk_size):
                    chunk = operacoes_para_inserir[i:i + insert_chunk_size]
                    
                    # Usar bulk_insert_mappings para melhor performance
                    db_bg.bulk_insert_mappings(OperacaoComex, chunk)
                    db_bg.commit()
                    
                    logger.info(f"  ‚úÖ Inseridos {min(i + insert_chunk_size, len(operacoes_para_inserir))}/{len(operacoes_para_inserir)} registros...")
                
                logger.success(f"‚úÖ Importa√ß√£o conclu√≠da: {stats['total_registros']} registros ({stats['importacoes']} importa√ß√µes, {stats['exportacoes']} exporta√ß√µes)")
                
            except Exception as e:
                logger.error(f"Erro no processamento em background: {e}")
                import traceback
                logger.error(traceback.format_exc())
            finally:
                db_bg.close()
                # Remover arquivo tempor√°rio
                try:
                    os.unlink(caminho_temp)
                    logger.info(f"üóëÔ∏è Arquivo tempor√°rio removido: {caminho_temp}")
                except:
                    pass
        
        # Iniciar processamento em thread separada
        thread = threading.Thread(target=processar_em_background, daemon=True)
        thread.start()
        
        # Retornar resposta imediata
        return {
            "success": True,
            "message": "Upload recebido. Processamento iniciado em background.",
            "arquivo": arquivo.filename,
            "status": "processando",
            "instrucoes": "Verifique os logs do Render para acompanhar o progresso. O processamento pode levar v√°rios minutos para arquivos grandes."
        }
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erro no upload: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"Erro no upload: {str(e)}")


# ============================================================================
# ENDPOINT 2: Upload e Importar CNAE
# ============================================================================

@app.post("/upload-e-importar-cnae", tags=["importacao"])
async def upload_e_importar_cnae(
    arquivo: UploadFile = File(..., description="Arquivo CNAE Excel (.xlsx ou .xls) para importar"),
    db: Session = Depends(get_db)
):
    """
    Faz upload de um arquivo CNAE Excel e importa automaticamente para o banco de dados.
    
    Aceita arquivos .xlsx e .xls via upload direto.
    OTIMIZADO: Usa bulk inserts para melhor performance.
    
    Colunas esperadas no Excel (aceita varia√ß√µes):
    - CNAE (ou C√≥digo CNAE, CNAE 2.0, Subclasse)
    - Descri√ß√£o (ou Descri√ß√£o Subclasse, Descri√ß√£o CNAE)
    - Classe (opcional)
    - Grupo (opcional)
    - Divis√£o (opcional)
    - Se√ß√£o (opcional)
    """
    try:
        # Validar extens√£o do arquivo
        nome_arquivo = arquivo.filename.lower()
        if not (nome_arquivo.endswith('.xlsx') or nome_arquivo.endswith('.xls')):
            raise HTTPException(
                status_code=400,
                detail="Arquivo deve ser Excel (.xlsx ou .xls)"
            )
        
        logger.info(f"üì§ Iniciando upload e importa√ß√£o do arquivo CNAE: {arquivo.filename}")
        
        # Salvar arquivo temporariamente
        with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx' if nome_arquivo.endswith('.xlsx') else '.xls') as tmp_file:
            conteudo = await arquivo.read()
            tmp_file.write(conteudo)
            caminho_temp = tmp_file.name
        
        try:
            # Ler Excel
            logger.info("üìñ Lendo arquivo Excel...")
            df = pd.read_excel(caminho_temp)
            logger.info(f"‚úÖ Arquivo lido: {len(df)} linhas, {len(df.columns)} colunas")
            logger.info(f"Colunas dispon√≠veis: {list(df.columns)}")
            
            stats = {
                "arquivo": arquivo.filename,
                "total_registros": 0,
                "inseridos": 0,
                "atualizados": 0,
                "erros": []
            }
            
            # Preparar lista para bulk insert
            cnae_para_inserir = []
            cnae_existentes = set()
            
            # Buscar CNAEs existentes uma √∫nica vez
            logger.info("üîç Verificando CNAEs existentes...")
            existentes_db = db.query(CNAEHierarquia.cnae).all()
            cnae_existentes = {row[0] for row in existentes_db}
            logger.info(f"  Encontrados {len(cnae_existentes)} CNAEs existentes")
            
            # Processar em batches
            batch_size = 500
            total_rows = len(df)
            
            logger.info(f"üîÑ Processando {total_rows} linhas em batches de {batch_size}...")
            
            for batch_start in range(0, total_rows, batch_size):
                batch_end = min(batch_start + batch_size, total_rows)
                batch_df = df.iloc[batch_start:batch_end]
                
                for idx, row in batch_df.iterrows():
                    try:
                        # Tentar diferentes nomes de colunas para CNAE
                        cnae = (
                            str(row.get('CNAE', '')) or
                            str(row.get('C√≥digo CNAE', '')) or
                            str(row.get('CNAE 2.0', '')) or
                            str(row.get('Subclasse', ''))
                        ).strip()
                        
                        if not cnae or cnae == 'nan' or len(cnae) < 4:
                            continue
                        
                        # Limpar CNAE (remover pontos, tra√ßos, etc)
                        cnae_limpo = cnae.replace('.', '').replace('-', '').strip()
                        
                        # Extrair informa√ß√µes adicionais
                        descricao = (
                            str(row.get('Descri√ß√£o', '')) or
                            str(row.get('Descri√ß√£o Subclasse', '')) or
                            str(row.get('Descri√ß√£o CNAE', ''))
                        ).strip()[:500]
                        
                        classe = str(row.get('Classe', '')).strip()[:10] if pd.notna(row.get('Classe')) else None
                        grupo = str(row.get('Grupo', '')).strip()[:10] if pd.notna(row.get('Grupo')) else None
                        divisao = str(row.get('Divis√£o', '')).strip()[:10] if pd.notna(row.get('Divis√£o')) else None
                        secao = str(row.get('Se√ß√£o', '')).strip()[:10] if pd.notna(row.get('Se√ß√£o')) else None
                        
                        # Verificar se j√° existe (usando set em mem√≥ria)
                        if cnae_limpo in cnae_existentes:
                            stats["atualizados"] += 1
                            # Para atualiza√ß√µes, precisamos fazer individualmente
                            existente = db.query(CNAEHierarquia).filter(
                                CNAEHierarquia.cnae == cnae_limpo
                            ).first()
                            if existente:
                                if descricao:
                                    existente.descricao = descricao
                                if classe:
                                    existente.classe = classe
                                if grupo:
                                    existente.grupo = grupo
                                if divisao:
                                    existente.divisao = divisao
                                if secao:
                                    existente.secao = secao
                        else:
                            # Adicionar √† lista para bulk insert
                            cnae_para_inserir.append({
                                'cnae': cnae_limpo,
                                'descricao': descricao,
                                'classe': classe,
                                'grupo': grupo,
                                'divisao': divisao,
                                'secao': secao
                            })
                            cnae_existentes.add(cnae_limpo)  # Adicionar ao set para evitar duplicatas
                            stats["inseridos"] += 1
                        
                        stats["total_registros"] += 1
                    
                    except Exception as e:
                        logger.error(f"Erro ao processar linha {idx}: {e}")
                        stats["erros"].append(f"Linha {idx}: {str(e)}")
                        continue
                
                # Log de progresso
                if batch_end % 1000 == 0 or batch_end == total_rows:
                    logger.info(f"  üìä Processadas {batch_end}/{total_rows} linhas... ({len(cnae_para_inserir)} novos, {stats['atualizados']} atualizados)")
            
            # Commit atualiza√ß√µes primeiro
            if stats["atualizados"] > 0:
                db.commit()
                logger.info(f"‚úÖ {stats['atualizados']} registros atualizados")
            
            # Bulk insert para novos registros
            if cnae_para_inserir:
                logger.info(f"üíæ Inserindo {len(cnae_para_inserir)} novos CNAEs no banco...")
                insert_chunk_size = 1000
                
                for i in range(0, len(cnae_para_inserir), insert_chunk_size):
                    chunk = cnae_para_inserir[i:i + insert_chunk_size]
                    db.bulk_insert_mappings(CNAEHierarquia, chunk)
                    db.commit()
                    logger.info(f"  ‚úÖ Inseridos {min(i + insert_chunk_size, len(cnae_para_inserir))}/{len(cnae_para_inserir)} registros...")
            
            logger.success(f"‚úÖ Importa√ß√£o de CNAE conclu√≠da: {stats['inseridos']} inseridos, {stats['atualizados']} atualizados")
            
            return {
                "success": True,
                "message": "Upload e importa√ß√£o de CNAE conclu√≠dos",
                "stats": stats
            }
        
        finally:
            # Remover arquivo tempor√°rio
            try:
                os.unlink(caminho_temp)
            except:
                pass
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erro no upload e importa√ß√£o de CNAE: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"Erro no upload e importa√ß√£o de CNAE: {str(e)}")


# ============================================================================
# INFORMA√á√ïES ADICIONAIS
# ============================================================================

"""
COMO USAR OS ENDPOINTS:

1. Via Swagger UI:
   - Acesse: https://comex-backend-gecp.onrender.com/docs
   - Procure: POST /upload-e-importar-excel ou POST /upload-e-importar-cnae
   - Clique em "Try it out"
   - Selecione o arquivo
   - Clique em "Execute"

2. Via curl (PowerShell):
   # Excel
   $filePath = "C:\Users\User\Desktop\Cursor\Projetos\projeto_comex\comex_data\comexstat_csv\H_EXPORTACAO_E IMPORTACAO_GERAL_2025-01_2025-12_DT20260107.xlsx"
   curl.exe -X POST "https://comex-backend-gecp.onrender.com/upload-e-importar-excel" -H "accept: application/json" -F "arquivo=@$filePath"
   
   # CNAE
   $filePath = "C:\Users\User\Desktop\Cursor\Projetos\projeto_comex\comex_data\comexstat_csv\CNAE.xlsx"
   curl.exe -X POST "https://comex-backend-gecp.onrender.com/upload-e-importar-cnae" -H "accept: application/json" -F "arquivo=@$filePath"

3. Via Python requests:
   import requests
   
   # Excel
   url = "https://comex-backend-gecp.onrender.com/upload-e-importar-excel"
   with open("arquivo.xlsx", "rb") as f:
       files = {"arquivo": f}
       response = requests.post(url, files=files)
   print(response.json())
   
   # CNAE
   url = "https://comex-backend-gecp.onrender.com/upload-e-importar-cnae"
   with open("CNAE.xlsx", "rb") as f:
       files = {"arquivo": f}
       response = requests.post(url, files=files)
   print(response.json())


FORMATO ESPERADO DOS ARQUIVOS:

Excel (Importa√ß√£o/Exporta√ß√£o):
- C√≥digo NCM (obrigat√≥rio)
- Descri√ß√£o NCM (opcional)
- UF do Produto (opcional)
- Pa√≠ses (opcional)
- M√™s (opcional, pode ser n√∫mero ou nome do m√™s)
- Exporta√ß√£o - 2025 - Valor US$ FOB (ou Exporta√ß√£o - Valor US$ FOB)
- Exporta√ß√£o - 2025 - Quilograma L√≠quido (ou Exporta√ß√£o - Quilograma L√≠quido)
- Importa√ß√£o - 2025 - Valor US$ FOB (ou Importa√ß√£o - Valor US$ FOB)
- Importa√ß√£o - 2025 - Quilograma L√≠quido (ou Importa√ß√£o - Quilograma L√≠quido)

CNAE:
- CNAE (ou C√≥digo CNAE, CNAE 2.0, Subclasse) - obrigat√≥rio
- Descri√ß√£o (ou Descri√ß√£o Subclasse, Descri√ß√£o CNAE) - opcional
- Classe (opcional)
- Grupo (opcional)
- Divis√£o (opcional)
- Se√ß√£o (opcional)


OTIMIZA√á√ïES IMPLEMENTADAS:

1. Processamento em Background (Excel):
   - Retorna resposta imediata para evitar timeout do Render (30s)
   - Processamento continua em thread separada
   - Logs detalhados para acompanhar progresso

2. Bulk Inserts:
   - Usa bulk_insert_mappings() em vez de inser√ß√µes individuais
   - Processa em chunks de 1000 registros
   - Reduz tempo de processamento em 50-100x

3. Processamento em Batches:
   - Processa arquivos grandes em batches de 500 linhas
   - Evita consumo excessivo de mem√≥ria
   - Logs de progresso a cada 1000 linhas

4. Verifica√ß√£o de Duplicatas Otimizada (CNAE):
   - Busca CNAEs existentes uma √∫nica vez
   - Usa set em mem√≥ria para verifica√ß√£o r√°pida
   - Atualiza√ß√µes feitas individualmente apenas quando necess√°rio
"""
